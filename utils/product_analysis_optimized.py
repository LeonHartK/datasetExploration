"""
An√°lisis de productos optimizado usando mlxtend
Mucho m√°s r√°pido para datasets grandes
"""

import pandas as pd
import numpy as np
from mlxtend.frequent_patterns import fpgrowth, association_rules as mlxtend_rules
from mlxtend.preprocessing import TransactionEncoder


def analyze_association_rules_optimized(
    df: pd.DataFrame,
    min_support: float = 0.01,
    min_confidence: float = 0.3,
    use_fpgrowth: bool = True,
    max_len: int = 3
) -> pd.DataFrame:
    """
    An√°lisis de reglas de asociaci√≥n OPTIMIZADO usando mlxtend

    Args:
        df: DataFrame transformado con productos_list
        min_support: Soporte m√≠nimo (0.01 = 1%)
        min_confidence: Confianza m√≠nima
        use_fpgrowth: Si True usa FP-Growth (r√°pido), si False usa Apriori
        max_len: Longitud m√°xima de itemsets (3 = triples m√°ximo)

    Returns:
        DataFrame con reglas de asociaci√≥n
    """
    print(f"\n{'='*70}")
    print(f"AN√ÅLISIS DE REGLAS DE ASOCIACI√ìN (OPTIMIZADO con {'FP-Growth' if use_fpgrowth else 'Apriori'})")
    print(f"{'='*70}")
    print(f"Par√°metros:")
    print(f"  ‚Ä¢ Soporte m√≠nimo: {min_support*100:.2f}%")
    print(f"  ‚Ä¢ Confianza m√≠nima: {min_confidence*100:.1f}%")
    print(f"  ‚Ä¢ Max longitud itemsets: {max_len}")

    # 1. Filtrar transacciones con productos
    df_with_products = df[df['tiene_productos']].copy()
    n_transactions = len(df_with_products)
    print(f"\nTransacciones con productos: {n_transactions:,}")

    # 2. Preparar transacciones como lista
    transactions = df_with_products['productos_list'].tolist()

    # Estad√≠sticas
    sizes = [len(t) for t in transactions]
    print(f"\nEstad√≠sticas de transacciones:")
    print(f"  ‚Ä¢ Productos/transacci√≥n (promedio): {np.mean(sizes):.2f}")
    print(f"  ‚Ä¢ Productos/transacci√≥n (mediana): {np.median(sizes):.0f}")

    # 3. Convertir a formato one-hot (matriz binaria)
    print(f"\nConvirtiendo a formato one-hot...")
    te = TransactionEncoder()
    te_ary = te.fit(transactions).transform(transactions)
    df_encoded = pd.DataFrame(te_ary, columns=te.columns_)

    print(f"  ‚Ä¢ Productos √∫nicos encontrados: {len(te.columns_):,}")
    print(f"  ‚Ä¢ Matriz de transacciones: {df_encoded.shape}")

    # 4. Encontrar itemsets frecuentes (FP-Growth o Apriori)
    print(f"\nBuscando itemsets frecuentes con {'FP-Growth' if use_fpgrowth else 'Apriori'}...")

    if use_fpgrowth:
        # FP-Growth es mucho m√°s r√°pido
        frequent_itemsets = fpgrowth(
            df_encoded,
            min_support=min_support,
            use_colnames=True,
            max_len=max_len
        )
    else:
        # Apriori (m√°s lento pero tambi√©n optimizado)
        from mlxtend.frequent_patterns import apriori
        frequent_itemsets = apriori(
            df_encoded,
            min_support=min_support,
            use_colnames=True,
            max_len=max_len
        )

    print(f"  ‚úì Itemsets frecuentes encontrados: {len(frequent_itemsets):,}")

    if len(frequent_itemsets) == 0:
        print("\n‚ö†Ô∏è  No se encontraron itemsets frecuentes con estos par√°metros")
        return pd.DataFrame()

    # Estad√≠sticas por longitud
    frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(len)
    length_counts = frequent_itemsets['length'].value_counts().sort_index()
    print(f"\nDistribuci√≥n por longitud:")
    for length, count in length_counts.items():
        print(f"  ‚Ä¢ {length}-itemsets: {count:,}")

    # 5. Generar reglas de asociaci√≥n
    print(f"\nGenerando reglas de asociaci√≥n...")
    rules = mlxtend_rules(
        frequent_itemsets,
        metric="confidence",
        min_threshold=min_confidence
    )

    if len(rules) == 0:
        print("\n‚ö†Ô∏è  No se encontraron reglas con estos par√°metros")
        return pd.DataFrame()

    print(f"  ‚úì Reglas encontradas: {len(rules):,}")

    # 6. Formatear resultados (nombres en espa√±ol para compatibilidad)
    rules_formatted = pd.DataFrame({
        'antecedente': rules['antecedents'].apply(lambda x: ', '.join(sorted(list(x)))),
        'consecuente': rules['consequents'].apply(lambda x: ', '.join(sorted(list(x)))),
        'soporte': rules['support'].round(4),
        'confianza': rules['confidence'].round(4),
        'lift': rules['lift'].round(2),
        'conviction': rules['conviction'].round(2) if 'conviction' in rules.columns else None,
        'num_transacciones': (rules['support'] * n_transactions).astype(int)
    })

    # Ordenar por lift descendente
    rules_formatted = rules_formatted.sort_values('lift', ascending=False).reset_index(drop=True)

    # Estad√≠sticas
    print(f"\nEstad√≠sticas de las reglas:")
    print(f"  ‚Ä¢ Confianza promedio: {rules_formatted['confianza'].mean():.3f}")
    print(f"  ‚Ä¢ Confianza mediana: {rules_formatted['confianza'].median():.3f}")
    print(f"  ‚Ä¢ Lift promedio: {rules_formatted['lift'].mean():.2f}")
    print(f"  ‚Ä¢ Lift m√°ximo: {rules_formatted['lift'].max():.2f}")

    # Top 10 reglas
    print(f"\nüìä Top 10 reglas por Lift:")
    print(rules_formatted[['antecedente', 'consecuente', 'confianza', 'lift']].head(10).to_string(index=False))

    return rules_formatted


def analyze_association_rules_sampled(
    df: pd.DataFrame,
    sample_frac: float = 0.1,
    **kwargs
) -> pd.DataFrame:
    """
    An√°lisis de reglas usando una MUESTRA del dataset
    √ötil para datasets muy grandes

    Args:
        df: DataFrame completo
        sample_frac: Fracci√≥n a muestrear (0.1 = 10%)
        **kwargs: Argumentos para analyze_association_rules_optimized
    """
    print(f"\n‚ö° MODO MUESTREO: Usando {sample_frac*100:.0f}% del dataset")
    print(f"   Total: {len(df):,} ‚Üí Muestra: {int(len(df)*sample_frac):,}")

    df_sample = df.sample(frac=sample_frac, random_state=42)
    return analyze_association_rules_optimized(df_sample, **kwargs)
